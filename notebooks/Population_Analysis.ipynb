{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paltas\n",
    "from paltas.Analysis import hierarchical_inference\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import emcee\n",
    "import corner\n",
    "import numba\n",
    "import os\n",
    "\n",
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Hierarchical Inference on a Population of Strong Lenses\n",
    "\n",
    "__Author:__ Sebastian Wagner-Carena\n",
    "\n",
    "__Goals: Understand how to run hierarchical inference on the network outputs using `paltas`__ \n",
    "\n",
    "__If you have not already done so, you will have to install `tensorflow` to run the Analysis module tools in this notebook. This notebook will also take advantage of the package `emcee`, although you you can use any sampler you like. For the plotting we will use `corner`.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our data\n",
    "\n",
    "To best illustrate how the hierarchical inference tools work, we'll generate a set of artificial network outputs. However, the pipeline outlined here will work equally well with true network outputs. To do this we need to simulate a) the prior we use for trianing and b) the information content of the image. We will focus only on the SHMF normalization and therefore assume that the information content of the image is very low compared to the prior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the true population mean and its scatter\n",
    "true_mean = 1e-3\n",
    "true_scatter = 3e-4\n",
    "\n",
    "# Assume some consistent, large noise for all of our lens images\n",
    "image_information_scatter = np.random.uniform(low=4e-3,high=6e-3,size=100)\n",
    "image_mean = true_mean + np.random.normal(size=100)*image_information_scatter + np.random.normal(size=100)*true_scatter\n",
    "\n",
    "# This in the mean and scatter of the information in each image, but remember our network predicts a posterior which means we need to multiply our likelihood\n",
    "# by the training prior\n",
    "train_mean = 2e-3\n",
    "train_scatter = 1e-3\n",
    "network_means = (image_mean * train_scatter**2 + train_mean*image_information_scatter**2)/(train_scatter**2 + image_information_scatter**2)\n",
    "network_scatter = np.sqrt((train_scatter**2 * image_information_scatter**2)/(train_scatter**2 + image_information_scatter**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the image information and the network outputs. The network outputs are dominated by the prior. Quickly, let's confirm that a) if we combined the image information we would get a likelihood consistent with with the true mean and b) if we combine the network outputs we would get a likelihood consistent with the training mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by combining all the image level information\n",
    "plt.figure(figsize=(10,8))\n",
    "colors = ['#66c2a5','#fc8d62','#8da0cb','#e78ac3']\n",
    "x = np.linspace(-1e-3,7e-3,1000)\n",
    "prod = np.zeros(len(x))\n",
    "for i in range(100):\n",
    "    prod += norm(loc = image_mean[i],scale=image_information_scatter[i]).logpdf(x)\n",
    "# Normalize the max to 1 for visualization\n",
    "prod -= np.max(prod)\n",
    "plt.plot(x,np.exp(prod),label='Image Information',c=colors[0],lw=4)\n",
    "\n",
    "# Now combine the network outputs\n",
    "prod = np.zeros(len(x))\n",
    "for i in range(100):\n",
    "    prod += norm(loc = network_means[i],scale=network_scatter[i]).logpdf(x)\n",
    "prod -= np.max(prod)\n",
    "plt.plot(x,np.exp(prod),label='Network Information',c=colors[1],lw=4)\n",
    "plt.axvline(true_mean,label='True Mean',c=colors[2],lw=4,ls='--')\n",
    "plt.axvline(train_mean,label='Training Mean',c=colors[3],lw=4,ls='--')\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel(r'$\\Sigma_\\mathrm{sub}$',fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note because there is scatter in our true values, the combination we did above is not statistically correct, but it gives us a good idea of the challenges we face. The information about the population mean exists in the sample, but it washed out by the prior in the posteriors our network is estimating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Inference\n",
    "\n",
    "Now we can use our hierarchical inference tools to combine the network outputs and attempt to infer the population mean and scatter of the SHMF normalization.\n",
    "\n",
    "All of the work gets done by `hierarchical_inference.ProbabilityClassAnalytical` and `emcee`. `ProbabilityClassAnalytical` needs to be initialized with the mean vector and covariance matrix defining the interim training distribution (this class assumed that every distribution being considered is a multivariate Gaussian, but there are other classes in `hierarchical_inference.py` that relax that assumption at the cost of computational time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a few of the parameters for our inference\n",
    "n_lenses = 100\n",
    "n_emcee_samps = int(1e4)\n",
    "burnin = int(1e3)\n",
    "chains_folder = 'notebook_data/'\n",
    "backend_path = 'example_chains.h5'\n",
    "chains_path = os.path.join(chains_folder,backend_path)\n",
    "\n",
    "# Load the predictions for the mean and covariance for our model. We'll have to do a little reshaping here since the code\n",
    "# expect an array of mean values and a precision matrix.\n",
    "y_pred = network_means[:n_lenses].reshape((n_lenses,1))\n",
    "prec_pred = 1/np.square(network_scatter[:n_lenses].reshape((n_lenses,1,1)))\n",
    "\n",
    "# The interim training distribution.\n",
    "mu_omega_i = np.array([train_mean])\n",
    "cov_omega_i =np.diag(np.array([train_scatter])**2)\n",
    "\n",
    "# We will want to initialize emcee near the correct values.\n",
    "mu_omega = np.array([true_mean])\n",
    "cov_omega =np.diag(np.array([true_scatter])**2)\n",
    "true_hyperparameters = np.concatenate([mu_omega,np.log(np.diag(np.sqrt(cov_omega)))])\n",
    "\n",
    "# A prior function that mainly just bounds the uncertainty estimation.\n",
    "@numba.njit()\n",
    "def eval_func_omega(hyperparameters):\n",
    "    # Enforce that the SHMF normalization is not negative\n",
    "    if hyperparameters[0] < 0:\n",
    "        return -np.inf\n",
    "    # Need to set bounds to avoid random singular matrix proposals\n",
    "    if hyperparameters[1] < -12:\n",
    "        return -np.inf\n",
    "    return 0\n",
    "\n",
    "# Initialize our class and then give it the network predictions. These are set to global variables in case you want to use\n",
    "# pooling.\n",
    "prob_class = hierarchical_inference.ProbabilityClassAnalytical(mu_omega_i,cov_omega_i,eval_func_omega)\n",
    "prob_class.set_predictions(mu_pred_array_input=y_pred,prec_pred_array_input=prec_pred)\n",
    "\n",
    "# Set a few of the parameters we will need to pass to emcee\n",
    "n_walkers = 40\n",
    "ndim = 2\n",
    "\n",
    "# Generate an initial state around the true values (this helps with convergence for this example) \n",
    "initial_std = np.array([5e-4,1])\n",
    "cur_state = (np.random.rand(n_walkers, ndim)*2-1)*initial_std\n",
    "cur_state += true_hyperparameters\n",
    "\n",
    "backend = emcee.backends.HDFBackend(chains_path)\n",
    "sampler = emcee.EnsembleSampler(n_walkers, ndim,prob_class.log_post_omega,backend=backend)\n",
    "sampler.run_mcmc(cur_state,n_emcee_samps,progress=True)\n",
    "chain = sampler.chain[:,burnin:,:].reshape((-1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the constraints using `corner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_param_print=[r'$\\Sigma_\\mathrm{sub,pop} \\times 10^{3}$' + '\\n' + r'$[\\mathrm{kpc}^{-2}]$',\n",
    "                    r'$\\log \\Sigma_\\mathrm{sub,pop,\\sigma} \\times 10^{3}$' + '\\n' + r'$[\\mathrm{kpc}^{-2}]$']\n",
    "fontsize = 20\n",
    "color='#FFAA00'\n",
    "truth_color = 'k'\n",
    "hist_kwargs = {'density':True,'color':color,'lw':3}\n",
    "\n",
    "corner.corner(chain,labels=corner_param_print,bins=20,show_titles=False,plot_datapoints=False,label_kwargs=dict(fontsize=fontsize),\n",
    "              levels=[0.68,0.95],color=color,fill_contours=True,hist_kwargs=hist_kwargs,title_fmt='.2f',truths=true_hyperparameters,\n",
    "              truth_color=truth_color,max_n_ticks=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have an unbiased inference of the mean, and it's clear from this data that the constrain on the scatter is an upper limit (not suprising given the small scatter and the large uncertainty of the information we assigned to each data point). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
