# -*- coding: utf-8 -*-
"""
Borrow some of the internal functions used to build ResNet in Keras.

This module implements some of the internal functions in Keras. They are
reimplemented here because their location in the keras code does not appear
to be stable yet.
"""

from tensorflow.keras import layers
from tensorflow.keras import backend


def block1(x, filters, kernel_size=3, stride=1,conv_shortcut=True, name=None):
	"""A residual block.

	# Arguments
		x: input tensor.
		filters: integer, filters of the bottleneck layer.
		kernel_size: default 3, kernel size of the bottleneck layer.
		stride: default 1, stride of the first layer.
		conv_shortcut: default True, use convolution shortcut if True,
			otherwise identity shortcut.
		name: string, block label.

	# Returns
		Output tensor for the residual block.
	"""
	bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1

	if conv_shortcut is True:
		shortcut = layers.Conv2D(4 * filters, 1, strides=stride,
			name=name + '_0_conv')(x)
		#shortcut = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,
		#	name=name + '_0_bn')(shortcut)
	else:
		shortcut = x

	x = layers.Conv2D(filters, 1, strides=stride, name=name + '_1_conv')(x)
	#x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,
	#	name=name + '_1_bn')(x)
	x = layers.Activation('relu', name=name + '_1_relu')(x)

	x = layers.Conv2D(filters, kernel_size, padding='SAME',
		name=name + '_2_conv')(x)
	#x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,
	#	name=name + '_2_bn')(x)
	x = layers.Activation('relu', name=name + '_2_relu')(x)

	x = layers.Conv2D(4 * filters, 1, name=name + '_3_conv')(x)
	#x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,
	#	name=name + '_3_bn')(x)

	x = layers.Add(name=name + '_add')([shortcut, x])
	x = layers.Activation('relu', name=name + '_out')(x)
	return x


def stack1(x, filters, blocks, stride1=2, name=None):
	"""A set of stacked residual blocks.

	# Arguments
		x: input tensor.
		filters: integer, filters of the bottleneck layer in a block.
		blocks: integer, blocks in the stacked blocks.
		stride1: default 2, stride of the first layer in the first block.
		name: string, stack label.

	# Returns
		Output tensor for the stacked blocks.
	"""
	x = block1(x, filters, stride=stride1, name=name + '_block1')
	for i in range(2, blocks + 1):
		x = block1(x, filters, conv_shortcut=False, name=name + '_block' +
			str(i))
	return x


def block2(x, filters, kernel_size=3, stride=1,conv_shortcut=False,
	name=None):
	"""A residual block.

	# Arguments
		x: input tensor.
		filters: integer, filters of the bottleneck layer.
		kernel_size: default 3, kernel size of the bottleneck layer.
		stride: default 1, stride of the first layer.
		conv_shortcut: default False, use convolution shortcut if True,
			otherwise identity shortcut.
		name: string, block label.

	# Returns
		Output tensor for the residual block.
	"""
	bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1

	preact = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,
		name=name + '_preact_bn')(x)
	preact = layers.Activation('relu', name=name + '_preact_relu')(preact)

	if conv_shortcut is True:
		shortcut = layers.Conv2D(4 * filters, 1, strides=stride,
			name=name + '_0_conv')(preact)
	else:
		shortcut = layers.MaxPooling2D(1, strides=stride)(x) if stride > 1 else x

	x = layers.Conv2D(filters, 1, strides=1, use_bias=False,
		name=name + '_1_conv')(preact)
	x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,
		name=name + '_1_bn')(x)
	x = layers.Activation('relu', name=name + '_1_relu')(x)

	x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)
	x = layers.Conv2D(filters, kernel_size, strides=stride,
		use_bias=False, name=name + '_2_conv')(x)
	x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,
		name=name + '_2_bn')(x)
	x = layers.Activation('relu', name=name + '_2_relu')(x)

	x = layers.Conv2D(4 * filters, 1, name=name + '_3_conv')(x)
	x = layers.Add(name=name + '_out')([shortcut, x])
	return x


def stack2(x, filters, blocks, stride1=2, name=None):
	"""A set of stacked residual blocks.

	# Arguments
		x: input tensor.
		filters: integer, filters of the bottleneck layer in a block.
		blocks: integer, blocks in the stacked blocks.
		stride1: default 2, stride of the first layer in the first block.
		name: string, stack label.

	# Returns
		Output tensor for the stacked blocks.
	"""
	x = block2(x, filters, conv_shortcut=True, name=name + '_block1')
	for i in range(2, blocks):
		x = block2(x, filters, name=name + '_block' + str(i))
	x = block2(x, filters, stride=stride1, name=name + '_block' + str(blocks))
	return x


def block3(x, filters, kernel_size=3, stride=1, groups=32,conv_shortcut=True,
	name=None):
	"""A residual block.

	# Arguments
		x: input tensor.
		filters: integer, filters of the bottleneck layer.
		kernel_size: default 3, kernel size of the bottleneck layer.
		stride: default 1, stride of the first layer.
		groups: default 32, group size for grouped convolution.
		conv_shortcut: default True, use convolution shortcut if True,
			otherwise identity shortcut.
		name: string, block label.

	# Returns
		Output tensor for the residual block.
	"""
	bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1

	if conv_shortcut is True:
		shortcut = layers.Conv2D((64 // groups) * filters, 1, strides=stride,
			use_bias=False, name=name + '_0_conv')(x)
		shortcut = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,
			name=name + '_0_bn')(shortcut)
	else:
		shortcut = x

	x = layers.Conv2D(filters, 1, use_bias=False, name=name + '_1_conv')(x)
	x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,
		name=name + '_1_bn')(x)
	x = layers.Activation('relu', name=name + '_1_relu')(x)

	c = filters // groups
	x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)
	x = layers.DepthwiseConv2D(kernel_size, strides=stride, depth_multiplier=c,
		use_bias=False, name=name + '_2_conv')(x)
	x_shape = backend.int_shape(x)[1:-1]
	x = layers.Reshape(x_shape + (groups, c, c))(x)
	output_shape = (x_shape + (groups, c) if backend.backend() == 'theano'
		else None)
	x = layers.Lambda(lambda x: sum([x[:, :, :, :, i] for i in range(c)]),
		output_shape=output_shape, name=name + '_2_reduce')(x)
	x = layers.Reshape(x_shape + (filters,))(x)
	x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,
		name=name + '_2_bn')(x)
	x = layers.Activation('relu', name=name + '_2_relu')(x)

	x = layers.Conv2D((64 // groups) * filters, 1, use_bias=False,
		name=name + '_3_conv')(x)
	x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,
		name=name + '_3_bn')(x)

	x = layers.Add(name=name + '_add')([shortcut, x])
	x = layers.Activation('relu', name=name + '_out')(x)
	return x


def stack3(x, filters, blocks, stride1=2, groups=32, name=None):
	"""A set of stacked residual blocks.

	# Arguments
		x: input tensor.
		filters: integer, filters of the bottleneck layer in a block.
		blocks: integer, blocks in the stacked blocks.
		stride1: default 2, stride of the first layer in the first block.
		groups: default 32, group size for grouped convolution.
		name: string, stack label.

	# Returns
		Output tensor for the stacked blocks.
	"""
	x = block3(x, filters, stride=stride1, groups=groups, name=name + '_block1')
	for i in range(2, blocks + 1):
		x = block3(x, filters, groups=groups, conv_shortcut=False,
			name=name + '_block' + str(i))
	return x
